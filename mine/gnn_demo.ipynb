{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/projects/kaggle_nesp/pse\")\n",
    "sys.path.append(\"/home/projects/kaggle_nesp/pse/nesp/pLM/esm\")\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "\n",
    "os.environ[\"GPU\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ssm2_meta_pkl = \"/home/data/02vip.datasets/metas/ssm2_meta.pkl\"\n",
    "# q6428_meta_pkl = \"/home/data/02vip.datasets/metas/q6428_meta.pkl\"\n",
    "\n",
    "# ssm2_meta = pd.read_pickle(ssm2_meta_pkl)\n",
    "# q6428_meta = pd.read_pickle(q6428_meta_pkl)\n",
    "\n",
    "# # replace\n",
    "# ssm2_meta.pdb_path = ssm2_meta.pdb_path.apply(lambda r: r.replace(\"/public/home/chengyifan/data\", \"/home/data\"))\n",
    "# ssm2_meta.wt_path = ssm2_meta.wt_path.apply(lambda r: r.replace(\"/public/home/chengyifan/data\", \"/home/data\"))\n",
    "\n",
    "# # check\n",
    "# ssm2_meta['tag'] = ssm2_meta.apply(\n",
    "#     lambda r: r.sequence[r.pos - 1] == r.mut,\n",
    "#     axis = 1\n",
    "# )\n",
    "\n",
    "# ssm2_meta['pdb_tag'] = ssm2_meta.pdb_path.apply(os.path.exists)\n",
    "# ssm2_meta['wt_tag'] = ssm2_meta.wt_path.apply(os.path.exists)\n",
    "\n",
    "# save_ssm2_meta = ssm2_meta[ssm2_meta.tag & ssm2_meta.pdb_tag & ssm2_meta.wt_tag]\n",
    "# # save_ssm2_meta.reset_index(drop=True).drop(columns=['tag', 'pdb_tag', 'wt_tag']).to_csv(\"./ssm2_meta.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data import load_wt_mut_pdb_pair\n",
    "from utils.misc import recursive_to\n",
    "\n",
    "import esm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import radius_graph\n",
    "from torch import nn\n",
    "from torch_geometric.nn import GCN2Conv, GCNConv, Sequential, global_max_pool, GATConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from Bio.PDB.Polypeptide import index_to_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssm2_meta_path = \"./ssm2_meta.csv\"\n",
    "\n",
    "ssm2_meta_df = pd.read_csv(\"./ssm2_meta.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True # slowly\n",
    "torch.backends.cudnn.benchmark = False\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class PairGraph(Data):\n",
    "    def __inc__(self, key: str, value: Any, *args, **kwargs) -> Any:\n",
    "        if key == \"edge_index_mut\":\n",
    "            return self.x_mut.size(0)\n",
    "        if key == \"edge_index_wt\":\n",
    "            return self.x_wt.size(0)\n",
    "        return super().__inc__(key, value, *args, **kwargs)\n",
    "    \n",
    "    def __cat_dim__(self, key: str, value: Any, *args, **kwargs) -> Any:\n",
    "        return super().__cat_dim__(key, value, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProtDataset(Dataset):\n",
    "    def __init__(self, meta_path, radius=5, graph_pt=None, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.meta_path = meta_path\n",
    "        self.radius = radius\n",
    "        self.graph_pt = graph_pt\n",
    "        print(device)\n",
    "        \n",
    "        meta_df = pd.read_csv(meta_path)\n",
    "        meta_df = self.__check_columns(meta_df)\n",
    "        \n",
    "        if graph_pt is not None:\n",
    "            print(\"Parameter `graph_pt` is not None, loading graph data file from {} ...\".format(graph_pt))\n",
    "            self.datasets = torch.load(graph_pt)\n",
    "            \n",
    "        else:\n",
    "            esmv2, batch_converter = ProtDataset.get_esm_model(device)\n",
    "            print(\"esmv2 initial done.\")\n",
    "            \n",
    "            self.datasets = []\n",
    "            for _, items in tqdm(meta_df.iterrows()):\n",
    "                seq, wt, pos, mut = items.sequence, items.wt, items.pos, items.mut\n",
    "                pdb_path, wt_path = items.pdb_path, items.wt_path\n",
    "                ddG = items.ddG\n",
    "                \n",
    "                batch = load_wt_mut_pdb_pair(wt_path, pdb_path)\n",
    "                \n",
    "                # check sequence\n",
    "                resseq = batch['mut']['aa'].cpu()\n",
    "                assert resseq.shape[0] == 1 and len(resseq.shape) == 2  # single chain\n",
    "                parse_seq = [index_to_one(x) for x in np.array(resseq[0])]\n",
    "                parse_seq = \"\".join(seq)\n",
    "                if parse_seq != seq:\n",
    "                    print(\"warnings: [MisMatch] the `sequence` in meta_df does not match the parsed `sequence` from pdb file. \\\n",
    "                                The sequence in meta is {}, the items has been skip.\".format(seq))\n",
    "                    continue\n",
    "                    \n",
    "                if parse_seq[pos-1] != mut:\n",
    "                    print(\"warnings: [MisMatch] residue mismatch on mutant position, the mutation residue {} from meta does not match \\\n",
    "                          the residue {} parsed from the pdb file at position {}.\".format(mut, parse_seq[pos-1], pos))\n",
    "                    continue\n",
    "                \n",
    "                wt_seq = parse_seq[:pos-1] + wt + parse_seq[pos:]\n",
    "                logits_wt, reprs_wt, contacts_wt = self.__esm2_infer(wt_seq, esmv2, batch_converter, device)\n",
    "                logits_mut, reprs_mut, contacts_mut = self.__esm2_infer(parse_seq, esmv2, batch_converter, device)\n",
    "                \n",
    "                self.datasets.append(self.__build_graph(batch, reprs_wt, reprs_mut, radius=radius), pos, ddG)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.datasets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        protgraph, mut_pos, target = self.datasets[idx]\n",
    "        \n",
    "        return protgraph, int(mut_pos), target\n",
    "    \n",
    "    def __build_graph(self, batch, wt_reprs, mut_reprs, radius):\n",
    "        \n",
    "        x_wt_pos_ca = batch['wt']['pos14'][0, :, 1, :]  # (N, 3) position of CA\n",
    "        x_mut_pos_ca = batch['mut']['pos14'][0, :, 1, :]\n",
    "        \n",
    "        wt_reprs = torch.Tensor(wt_reprs[0, 1:-1, :]) # (N, V)\n",
    "        mut_reprs = torch.Tensor(mut_reprs[0, 1:-1, :])\n",
    "        # assert x_wt_pos_ca.shape[0] == wt_reprs.shape[0] and x_mut_pos_ca.shape[0] == mut_reprs.shape[0]\n",
    "        \n",
    "        edge_index_mut = radius_graph(x=x_mut_pos_ca, r=radius)\n",
    "        edge_index_wt = radius_graph(x=x_wt_pos_ca, r=radius)\n",
    "        \n",
    "        data = PairGraph(x_mut=mut_reprs, x_wt=wt_reprs, edge_index_mut=edge_index_mut, edge_index_wt=edge_index_wt)\n",
    "\n",
    "        return data\n",
    "        \n",
    "    def __check_columns(self, meta_df):\n",
    "        cols = ['sequence', 'mut', 'pos', 'wt', 'pdb_path', 'wt_path', 'ddG']\n",
    "        for col in cols:\n",
    "            assert col in meta_df.columns, \"column {} not in meta df.\".format(col)\n",
    "        \n",
    "        # check pos\n",
    "        for i, row in meta_df.iterrows():\n",
    "            seq, pos, mut = row.sequence, row.pos, row.mut\n",
    "            assert seq[pos - 1] == mut, \"position error, residue {} in sequence at position {} does not match residue {} in meta df.\".format(seq[pos-1], pos, mut)\n",
    "        \n",
    "        return meta_df\n",
    "    \n",
    "    def __esm2_infer(self, sequence, esmv2, batch_converter, device):\n",
    "        data = [(\"protein1\", sequence)]\n",
    "        batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "        batch_tokens = batch_tokens.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            results = esmv2.forward(batch_tokens, repr_layers=[33], need_head_weights=True, return_contacts=True)\n",
    "        logits = results['logits'].detach().cpu().numpy()\n",
    "        reps = results['representations'][33].detach().cpu().numpy()\n",
    "        contacts = results['contacts'].detach().cpu().numpy()\n",
    "        \n",
    "        return logits, reps, contacts\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_esm_model(device):\n",
    "        t_model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "        batch_converter = alphabet.get_batch_converter()\n",
    "        t_model.eval()  \n",
    "        t_model.to(device)\n",
    "        \n",
    "        return t_model, batch_converter\n",
    "    \n",
    "    def save_graphs(self, save_dir):\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "            \n",
    "        name = os.path.basename(self.meta_path).split(\".\")[0] + \"_r\" + str(self.radius) + \".pt\"\n",
    "        save_path = os.path.join(save_dir, name)\n",
    "        \n",
    "        torch.save(self.datasets, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Parameter `graph_pt` is not None, loading graph data file from ../data/ssm2_meta_r5_train.pt ...\n",
      "cuda\n",
      "Parameter `graph_pt` is not None, loading graph data file from ../data/ssm2_meta_r5_test.pt ...\n"
     ]
    }
   ],
   "source": [
    "# ssm2_dataset_r5 = ProtDataset(meta_path=ssm2_meta_path, radius=5, graph_pt=\"../data/ssm2_meta_r5.pt\")\n",
    "# ssm2_dataset_r6 = ProtDataset(meta_path=ssm2_meta_path, radius=6, graph_pt=\"../data/ssm2_meta_r6.pt\", device=device)\n",
    "\n",
    "ssm2_trainset_r5 = ProtDataset(meta_path=\"../data/ssm2_meta_r5_train.csv\", radius=5, graph_pt=\"../data/ssm2_meta_r5_train.pt\")\n",
    "ssm2_testset_r5 = ProtDataset(meta_path='../data/ssm2_meta_r5_test.csv', radius=5, graph_pt=\"../data/ssm2_meta_r5_test.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split train val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PairGraphBatch(x_mut=[5504, 1280], x_mut_batch=[5504], x_mut_ptr=[129], x_wt=[5504, 1280], x_wt_batch=[5504], x_wt_ptr=[129], edge_index_mut=[2, 14016], edge_index_wt=[2, 14122])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = DataLoader(ssm2_trainset_r5, batch_size=128, shuffle=True, follow_batch=['x_mut', 'x_wt'])\n",
    "val_loader = DataLoader(ssm2_testset_r5, batch_size=128, shuffle=True, follow_batch=['x_mut', 'x_wt'])\n",
    "batch = next(iter(train_loader))\n",
    "prots, mut_pos, targets = batch\n",
    "prots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProtMutNet(nn.Module):\n",
    "    def __init__(self, gnn_mode=\"GAT\") -> None:\n",
    "        super().__init__()\n",
    "        if gnn_mode == \"GAT\":\n",
    "            self.graph_encoder = Sequential('x, edge_index', [\n",
    "                (GATConv(1280, 512, head=4), 'x, edge_index -> x'),\n",
    "                nn.ReLU(inplace=True),\n",
    "                (GATConv(512, 256, head=4), 'x, edge_index -> x'),\n",
    "                nn.ReLU(inplace=True),\n",
    "                (GATConv(256, 128, head=4), 'x, edge_index -> x'),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ])\n",
    "        elif gnn_mode == \"GCN\":\n",
    "            self.graph_encoder = Sequential('x, edge_index', [\n",
    "                (GCNConv(1280, 512), 'x, edge_index -> x'),\n",
    "                nn.ReLU(inplace=True),\n",
    "                (GCNConv(512, 256), 'x, edge_index -> x'),\n",
    "                nn.ReLU(inplace=True),\n",
    "                (GCNConv(256, 128), 'x, edge_index -> x'),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ])\n",
    "        else:\n",
    "            raise\n",
    "        \n",
    "        # self.local_project = nn.Sequential([nn.Linear(128, 128), nn.ReLU(inplace=True)])\n",
    "        # self.global_project = nn.Sequential([nn.Linear(128, 128), nn.ReLU(inplace=True)])\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout1d(p=0.3),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, pairgraphs, mut_pos):\n",
    "        prot_mut_embeds = self.graph_encoder(pairgraphs.x_mut, pairgraphs.edge_index_mut)\n",
    "        prot_wt_embeds = self.graph_encoder(pairgraphs.x_wt, pairgraphs.edge_index_wt)\n",
    "        \n",
    "        # whole residue embeds\n",
    "        global_mut_embeds = global_max_pool(prot_mut_embeds, batch=pairgraphs.x_mut_batch)\n",
    "        global_wt_embeds = global_max_pool(prot_wt_embeds, batch=pairgraphs.x_wt_batch)\n",
    "        \n",
    "        # mutation amino acid embeds\n",
    "        mut_pos_ids = mut_pos + pairgraphs.x_mut_ptr[:-1]\n",
    "        wt_pos_ids = mut_pos + pairgraphs.x_wt_ptr[:-1]\n",
    "        local_mut_embeds = prot_mut_embeds[mut_pos_ids]\n",
    "        local_wt_embeds = prot_wt_embeds[wt_pos_ids]\n",
    "        \n",
    "        diff_embeds = torch.concat([global_mut_embeds, local_mut_embeds], dim=1) - torch.concat([global_wt_embeds, local_wt_embeds], dim=1)\n",
    "        \n",
    "        # prot_embeds_diff = torch.concat([local_mut_embeds, local_wt_embeds], dim=1)\n",
    "        \n",
    "        out = self.head(diff_embeds)\n",
    "        \n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_net = ProtMutNet(gnn_mode=\"GCN\")\n",
    "gat_net = ProtMutNet(gnn_mode=\"GAT\")\n",
    "\n",
    "net = gat_net\n",
    "\n",
    "net.to(device=device)\n",
    "\n",
    "optim = torch.optim.Adam(net.parameters() ,lr=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=2, gamma=0.98)\n",
    "loss_func = torch.nn.SmoothL1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch ==> 0, lr ==>0.00046, mean_mae ==> 0.16887036619745954, pearsonr ==> 0.7867216985958094\n",
      "              test_mean_mae ==> 0.13537550474015567, test_pearsonr ==> 0.8876148007538485, test_spearsonr ==> 0.8200148049406488\n",
      "epoch ==> 1, lr ==>0.00046, mean_mae ==> 0.16349446378192123, pearsonr ==> 0.7940141029964163\n",
      "              test_mean_mae ==> 0.1582452654838562, test_pearsonr ==> 0.8883140342721183, test_spearsonr ==> 0.8046316852195086\n",
      "epoch ==> 2, lr ==>0.00045, mean_mae ==> 0.16993432963381008, pearsonr ==> 0.7825307761692307\n",
      "              test_mean_mae ==> 0.1394504331625425, test_pearsonr ==> 0.8883455822383632, test_spearsonr ==> 0.8040453511622456\n",
      "epoch ==> 3, lr ==>0.00045, mean_mae ==> 0.15784757219407022, pearsonr ==> 0.8007719411116674\n",
      "              test_mean_mae ==> 0.1451554332788174, test_pearsonr ==> 0.880785375424649, test_spearsonr ==> 0.8012034875532902\n",
      "epoch ==> 4, lr ==>0.00044, mean_mae ==> 0.1552451336566283, pearsonr ==> 0.8033900883231406\n",
      "              test_mean_mae ==> 0.13803055653205284, test_pearsonr ==> 0.8885977175092865, test_spearsonr ==> 0.8016537460217908\n",
      "epoch ==> 5, lr ==>0.00044, mean_mae ==> 0.1506082325869677, pearsonr ==> 0.8068807814123967\n",
      "              test_mean_mae ==> 0.14204719089544737, test_pearsonr ==> 0.8829551988383416, test_spearsonr ==> 0.8031236433624654\n",
      "epoch ==> 6, lr ==>0.00043, mean_mae ==> 0.15434145608118602, pearsonr ==> 0.8023269953322694\n",
      "              test_mean_mae ==> 0.13512649673681992, test_pearsonr ==> 0.8896893331081949, test_spearsonr ==> 0.8155832476989462\n",
      "epoch ==> 7, lr ==>0.00043, mean_mae ==> 0.1593132700238909, pearsonr ==> 0.7917351177232227\n",
      "              test_mean_mae ==> 0.13539859251334116, test_pearsonr ==> 0.8898784750476204, test_spearsonr ==> 0.8063113557244522\n",
      "epoch ==> 8, lr ==>0.00043, mean_mae ==> 0.1530977373524588, pearsonr ==> 0.8006487397687563\n",
      "              test_mean_mae ==> 0.1486724202449505, test_pearsonr ==> 0.8862410427254733, test_spearsonr ==> 0.8041630312581413\n",
      "epoch ==> 9, lr ==>0.00043, mean_mae ==> 0.1463343581678916, pearsonr ==> 0.8120045298868642\n",
      "              test_mean_mae ==> 0.14026741568858808, test_pearsonr ==> 0.8899141270203377, test_spearsonr ==> 0.8161565056547947\n",
      "epoch ==> 10, lr ==>0.00042, mean_mae ==> 0.15003199981791632, pearsonr ==> 0.8103763173297536\n",
      "              test_mean_mae ==> 0.1225917711853981, test_pearsonr ==> 0.8902156990086088, test_spearsonr ==> 0.8138290591509342\n",
      "epoch ==> 11, lr ==>0.00042, mean_mae ==> 0.15126022620468724, pearsonr ==> 0.8026309726264393\n",
      "              test_mean_mae ==> 0.15054069745999116, test_pearsonr ==> 0.8881259291777246, test_spearsonr ==> 0.8147240961353167\n"
     ]
    }
   ],
   "source": [
    "from sklearn.isotonic import spearmanr, spearmanr\n",
    "\n",
    "\n",
    "train_epochs = 50\n",
    "\n",
    "for epoch in range(train_epochs):\n",
    "    if epoch != 0:\n",
    "        scheduler.step()\n",
    "    \n",
    "    # train\n",
    "    losses = 0.   \n",
    "    total_preds = []\n",
    "    total_targets = [] \n",
    "    net.train()\n",
    "    for batch in train_loader:\n",
    "        prots, mut_pos, targets = batch\n",
    "        \n",
    "        prots = prots.to(device)\n",
    "        targets = targets.to(device)\n",
    "        mut_pos = mut_pos.to(device)\n",
    "        \n",
    "        preds = net(prots, mut_pos=mut_pos)\n",
    "        \n",
    "        loss = loss_func(preds.flatten(), targets)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        losses += loss.item()\n",
    "        \n",
    "        total_preds.extend(preds.cpu().detach().numpy().flatten().tolist())\n",
    "        total_targets.extend(targets.cpu().numpy().tolist())\n",
    "    \n",
    "    mean_mae = losses / len(train_loader)\n",
    "    pr, p_value = pearsonr(total_targets, total_preds)\n",
    "    lr = [x['lr'] for x in optim.param_groups][0]\n",
    "    print(\"epoch ==> {}, lr ==>{:.5f}, mean_mae ==> {}, pearsonr ==> {}\".format(epoch, lr, mean_mae, pr))\n",
    "    \n",
    "    # test\n",
    "    test_losses = 0.   \n",
    "    total_preds = []\n",
    "    total_targets = [] \n",
    "    net.eval()\n",
    "    for batch in val_loader:\n",
    "        prots, mut_pos, targets = batch\n",
    "        \n",
    "        prots = prots.to(device)\n",
    "        targets = targets.to(device)\n",
    "        mut_pos = mut_pos.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            preds = net(prots, mut_pos=mut_pos)\n",
    "        \n",
    "        loss = loss_func(preds.flatten(), targets)\n",
    "        test_losses += loss.item()\n",
    "        \n",
    "        total_preds.extend(preds.cpu().detach().numpy().flatten().tolist())\n",
    "        total_targets.extend(targets.cpu().numpy().tolist())\n",
    "    \n",
    "    mean_mae = test_losses / len(val_loader)\n",
    "    pr, p_value = pearsonr(total_targets, total_preds)\n",
    "    sr, p_value = spearmanr(total_targets, total_preds)\n",
    "    print(\"              test_mean_mae ==> {}, test_pearsonr ==> {}, test_spearsonr ==> {}\".format(mean_mae, pr, sr))\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import OGB_MAG\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch_geometric.nn import HeteroConv, Linear, SAGEConv\n",
    "from torch_geometric.utils import trim_to_layer\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--device', type=str, default='cuda')\n",
    "# parser.add_argument('--use-sparse-tensor', action='store_true')\n",
    "# args = parser.parse_args()\n",
    "use_sparse_tensor = True\n",
    "device = \"cuda\"\n",
    "\n",
    "device = device if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "transforms = [T.ToUndirected(merge=True)]\n",
    "if use_sparse_tensor:\n",
    "    transforms.append(T.ToSparseTensor())\n",
    "dataset = OGB_MAG(root='/home/data/tmp', preprocess='metapath2vec',\n",
    "                  transform=T.Compose(transforms))\n",
    "data = dataset[0].to(device, 'x', 'y')\n",
    "\n",
    "\n",
    "class HierarchicalHeteroGraphSage(torch.nn.Module):\n",
    "    def __init__(self, edge_types, hidden_channels, out_channels, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            conv = HeteroConv(\n",
    "                {\n",
    "                    edge_type: SAGEConv((-1, -1), hidden_channels)\n",
    "                    for edge_type in edge_types\n",
    "                }, aggr='sum')\n",
    "            self.convs.append(conv)\n",
    "\n",
    "        self.lin = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict, num_sampled_edges_dict,\n",
    "                num_sampled_nodes_dict):\n",
    "\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x_dict, edge_index_dict, _ = trim_to_layer(\n",
    "                layer=i,\n",
    "                num_sampled_nodes_per_hop=num_sampled_nodes_dict,\n",
    "                num_sampled_edges_per_hop=num_sampled_edges_dict,\n",
    "                x=x_dict,\n",
    "                edge_index=edge_index_dict,\n",
    "            )\n",
    "\n",
    "            x_dict = conv(x_dict, edge_index_dict)\n",
    "            x_dict = {key: x.relu() for key, x in x_dict.items()}\n",
    "\n",
    "        return self.lin(x_dict['paper'])\n",
    "\n",
    "\n",
    "model = HierarchicalHeteroGraphSage(\n",
    "    edge_types=data.edge_types,\n",
    "    hidden_channels=64,\n",
    "    out_channels=dataset.num_classes,\n",
    "    num_layers=2,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "kwargs = {'batch_size': 1024, 'num_workers': 0}\n",
    "train_loader = NeighborLoader(\n",
    "    data,\n",
    "    num_neighbors=[10] * 2,\n",
    "    shuffle=True,\n",
    "    input_nodes=('paper', data['paper'].train_mask),\n",
    "    **kwargs,\n",
    ")\n",
    "\n",
    "val_loader = NeighborLoader(\n",
    "    data,\n",
    "    num_neighbors=[10] * 2,\n",
    "    shuffle=False,\n",
    "    input_nodes=('paper', data['paper'].val_mask),\n",
    "    **kwargs,\n",
    ")\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_examples = total_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = model(\n",
    "            batch.x_dict,\n",
    "            batch.adj_t_dict\n",
    "            if use_sparse_tensor else batch.edge_index_dict,\n",
    "            num_sampled_nodes_dict=batch.num_sampled_nodes_dict,\n",
    "            num_sampled_edges_dict=batch.num_sampled_edges_dict,\n",
    "        )\n",
    "\n",
    "        batch_size = batch['paper'].batch_size\n",
    "        loss = F.cross_entropy(out[:batch_size], batch['paper'].y[:batch_size])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_examples += batch_size\n",
    "        total_loss += float(loss) * batch_size\n",
    "\n",
    "    return total_loss / total_examples\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    total_examples = total_correct = 0\n",
    "    for batch in tqdm(loader):\n",
    "        batch = batch.to(device)\n",
    "        out = model(\n",
    "            batch.x_dict,\n",
    "            batch.adj_t_dict\n",
    "            if use_sparse_tensor else batch.edge_index_dict,\n",
    "            num_sampled_nodes_dict=batch.num_sampled_nodes_dict,\n",
    "            num_sampled_edges_dict=batch.num_sampled_edges_dict,\n",
    "        )\n",
    "\n",
    "        batch_size = batch['paper'].batch_size\n",
    "        pred = out[:batch_size].argmax(dim=-1)\n",
    "        total_examples += batch_size\n",
    "        total_correct += int((pred == batch['paper'].y[:batch_size]).sum())\n",
    "\n",
    "    return total_correct / total_examples\n",
    "\n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    loss = train()\n",
    "    val_acc = test(val_loader)\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Val: {val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = next(iter(train_loader))\n",
    "\n",
    "val_data = next(iter(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
